from sklearn.feature_extraction.text import CountVectorizer
from bs4 import BeautifulSoup
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import RegexpTokenizer
import pymystem3
import json

stop_words = ["еще", "него", "сказать", "а", "ж", "нее", "со", "без", "же", "ней", "совсем", "более", "жизнь", "нельзя", "так", "больше", "за", "нет", "такой", "будет", "зачем", "ни", "там", "будто", "здесь", "нибудь", "тебя", "бы", "и", "никогда", "тем", "был", "из", "ним", "теперь", "была", "из", "за", "них", "то", "были", "или", "ничего", "тогда", "было", "им", "но", "того", "быть", "иногда", "ну", "тоже", "в", "их", "о", "только", "вам", "к", "об", "том", "вас", "кажется", "один", "тот", "вдруг", "как", "он", "три", "ведь", "какая", "она", "тут", "во", "какой", "они", "ты", "вот", "когда", "опять", "у", "впрочем", "конечно", "от", "уж", "все", "которого", "перед", "уже", "всегда", "которые", "по", "хорошо", "всего", "кто", "под", "хоть", "всех", "куда", "после", "чего", "всю", "ли", "потом", "человек", "вы", "лучше", "потому", "чем", "г", "между", "почти", "через", "где", "меня", "при", "что", "говорил", "мне", "про", "чтоб", "да", "много", "раз", "чтобы", "даже", "может", "разве", "чуть", "два", "можно", "с", "эти", "для", "мой", "сам", "этого", "до", "моя", "свое", "этой", "другой", "мы", "свою", "этом", "его", "на", "себе", "этот", "ее", "над", "себя", "эту", "ей", "надо", "сегодня", "я", "ему", "наконец", "сейчас", "если", "нас", "сказал", "есть", "не", "сказала"]
tokenizer = RegexpTokenizer(u'[A-Za-zА-Яа-яёЁ]+')
m = pymystem3.Mystem()

def tokenize(text):
    tokens = tokenizer.tokenize(text)
    lemms = []
    for token in tokens:
        if len(token) > 2:
            lemms.append(m.lemmatize(token)[0])
    return lemms

texts = []
for i in range(1,21):
    f = open('base/base_' + str(i) + '.txt', 'r')
    html = f.readlines()
    f.close()
    text = BeautifulSoup(html[0], 'html.parser').text
    texts.append(text)

for i in range(1,3961):
    f = open('test/test_' + str(i) + '.txt', 'r')
    html = f.readlines()
    f.close()
    text = BeautifulSoup(html[0], 'html.parser').text
    texts.append(text)

cv = CountVectorizer(ngram_range=[1,2], stop_words=stop_words, tokenizer=tokenize, max_features=1000)
matrix = cv.fit_transform(texts)
mx = matrix.toarray()

cos_res = cosine_similarity(mx[:20], mx[20:])

my = [2561, 4, 3085, 3086, 598, 3603, 534, 1559, 25, 3610, 2076, 3102, 22, 2602, 2603, 45, 1072, 3123, 1076, 2101, 1593,
      2526, 572, 63, 3649, 2628, 586, 2635, 588, 2126, 2639, 1550, 3665, 85, 1622, 1111, 1115, 1723, 1632, 1635, 612,
      1638, 615, 3178, 2158, 2159, 1136, 1137, 3699, 3703, 2682, 636, 2173, 2174, 3712, 1666, 3715, 645, 2183, 1160,
      2185, 3722, 2187, 3212, 110, 3729, 1170, 2708, 3222, 2200, 153, 1690, 668, 2206, 3235, 1700, 711, 3239, 3240,
      3244, 174, 1711, 2224, 1201, 2228, 695, 2744, 1209, 1211, 1726, 3264, 3265, 2758, 3783, 712, 715, 1228, 2582,
      1571, 724, 3800, 219, 3804, 3293, 2939, 3808, 2275, 740, 1769, 748, 3822, 1779, 3830, 1273, 1152, 766, 1279, 3842,
      771, 1798, 2828, 2891, 3854, 3343, 2324, 792, 1825, 3292, 808, 812, 2782, 307, 1019, 312, 3125, 3387, 832, 3931,
      3906, 1347, 1348, 482, 3913, 1355, 3299, 3920, 1361, 2386, 2901, 857, 859, 1885, 3429, 2406, 1384, 364, 337, 2930,
      1911, 1991, 2427, 2950, 1929, 2954, 3468, 910, 1936, 402, 2884, 2544, 2463, 421, 1961, 939, 2760, 2480, 2481,
      2634, 444, 2232, 758, 3009, 450, 783, 455, 3022, 464, 2003, 3175, 2981, 3038, 3409, 2812, 2028, 1006, 3056, 3065,
      3925, 836, 2560]
defined = []
other = []
vacs = sum(cos_res)

i = 0

for vac in vacs:
    i += 1
    if (i in my):
        if (vac >= 4.5):
            defined.append(i)
        else:
            other.append(i)

result = {
    'defined': defined,
    'other': other
}

with open('lab4.json', 'w') as outfile:
    json.dump(result, outfile)